Advanced NLP Project: Question Answering with DistilBERT & RoBERTa

ğŸ“Œ Overview

This project explores and deploys Question Answering (QA) Language Models using Hugging Face Transformers.
It consists of two main parts:

Analysis Notebook (Advance_Level_final.ipynb) â€” in-depth exploration and comparison of DistilBERT and RoBERTa.

Streamlit App (streamlit_app.py) â€” interactive demo that allows users to input context and questions, and receive model-generated answers.

The project aligns with advanced NLP/ML tasks: implementation, exploration, visualization, research questions, and deployment.

âš™ï¸ Models Used

DistilBERT â†’ distilbert-base-cased-distilled-squad

RoBERTa â†’ deepset/roberta-base-squad2

Both are extractive QA models fine-tuned on the SQuAD dataset.

ğŸš€ How to Run
1. Notebook (Analysis)

Run the notebook in Google Colab or locally:

pip install -r requirements.txt
jupyter notebook Advance_Level_final.ipynb


Or open directly in Google Colab:
ğŸ‘‰ Open Advance_Level_final.ipynb in Colab

2. Streamlit App (Deployment)

Run locally:

pip install -r requirements.txt
streamlit run streamlit_app.py


Deploy on Streamlit Cloud
 or Hugging Face Spaces
.

ğŸ” Research Questions Explored

Which model is more confident overall?
â†’ RoBERTa is usually more confident.

Do models give consistent answers?
â†’ Both identify lunar maria and lava correctly, though phrasing may differ.

How do models handle myth-based questions?
â†’ Both can extract text about legends (e.g., people once believed the Moon had figures).

ğŸ“Š Key Insights

DistilBERT: Lightweight, fast, but less nuanced.

RoBERTa: Stronger, more confident, but heavier.

Both are limited to extractive QA (they canâ€™t invent answers beyond context).

Applications: education, research, quick knowledge retrieval.

Future: hybrid systems combining extractive + generative models.

ğŸ™Œ Credits

Hugging Face Transformers

Streamlit

PyTorch
